{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dWJkJHqfd607"
   },
   "outputs": [],
   "source": [
    "#Inspired by https://jnolis.com/blog/banned_license_plates/\n",
    "#Following tutorial here: https://www.tensorflow.org/text/tutorials/text_generation\n",
    "\n",
    "#load libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uitdPc2Lh-WB",
    "outputId": "9fb0d310-2a80-486c-8e4a-ba2d42398589"
   },
   "outputs": [],
   "source": [
    "#Get text of Ulysses from Project Gutenberg\n",
    "\n",
    "path_to_file = tf.keras.utils.get_file('ulysses.txt', 'https://www.gutenberg.org/files/4300/4300-0.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e_c-I48sie-1",
    "outputId": "2a1865dc-a3ab-482e-b5fc-f7052f322c1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1572272 characters\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NXTZyYutiyTR",
    "outputId": "c26f4cec-d3e0-4273-9adf-2f2dfbbbdcb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The Project Gutenberg eBook of Ulysses, by James Joyce\r\n",
      "\r\n",
      "This eBook is for the use of anyone anywhere in the United States and\r\n",
      "most other parts of the world at no cost and with almost no restrictions\r\n",
      "whatsoever. You may copy it, give it away or r\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_nbtQ1_tizv3",
    "outputId": "3239b32c-bc0c-4c0d-fac6-20c3b52058e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TG6cVGQWi2sz"
   },
   "outputs": [],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "absrq5ZIjDZP"
   },
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "JKtuYC6mjG8c"
   },
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VpYjJ1z1jew-",
    "outputId": "b2a3853b-0f80-45b4-91ba-f52f3d4f85d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1572272,), dtype=int64, numpy=array([126,  51,  68, ...,   1,   2,   1])>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "1dCq40zijhDD"
   },
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LV6gFhaJjjQp",
    "outputId": "aa0cd25e-69e5-4727-ea72-c2bfc27a6354"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿\n",
      "T\n",
      "h\n",
      "e\n",
      " \n",
      "P\n",
      "r\n",
      "o\n",
      "j\n",
      "e\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "o6PfhJLJjk8H"
   },
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "-bQqD3u5jnTd"
   },
   "outputs": [],
   "source": [
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "OpTvUMUwjpaP"
   },
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "lz7asnRwjv7r"
   },
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lIoNX375jxqi",
    "outputId": "abafe316-4626-4c30-ac45-81d819281b76"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "PYrm6FLHj2I-"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "xncBBsawj5zF"
   },
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "kOqrIExqj7rS"
   },
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tz6-9MtLj9Vi",
    "outputId": "c1d23de7-f4e0-4b89-f4e8-7565680d9773"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 127) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fpSzA_7wj_GF",
    "outputId": "22bda7eb-ff65-4efe-af65-6f45a5b002ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  32512     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  130175    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,100,991\n",
      "Trainable params: 4,100,991\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "zC3lNILukCBD"
   },
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "DCJItglYkGH0"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "evo5fVxMkHP3"
   },
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "aLKib6ATkPE8"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zoN9zHB9kSKW",
    "outputId": "c0d8645b-4130-4bed-c67d-b015baf24d2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "243/243 [==============================] - 460s 2s/step - loss: 2.6512\n",
      "Epoch 2/30\n",
      "243/243 [==============================] - 441s 2s/step - loss: 2.0296\n",
      "Epoch 3/30\n",
      "243/243 [==============================] - 436s 2s/step - loss: 1.8024\n",
      "Epoch 4/30\n",
      "243/243 [==============================] - 456s 2s/step - loss: 1.6637\n",
      "Epoch 5/30\n",
      "243/243 [==============================] - 440s 2s/step - loss: 1.5724\n",
      "Epoch 6/30\n",
      "243/243 [==============================] - 437s 2s/step - loss: 1.5045\n",
      "Epoch 7/30\n",
      "243/243 [==============================] - 454s 2s/step - loss: 1.4458\n",
      "Epoch 8/30\n",
      "243/243 [==============================] - 444s 2s/step - loss: 1.3941\n",
      "Epoch 9/30\n",
      "243/243 [==============================] - 432s 2s/step - loss: 1.3444\n",
      "Epoch 10/30\n",
      "243/243 [==============================] - 427s 2s/step - loss: 1.2950\n",
      "Epoch 11/30\n",
      "243/243 [==============================] - 427s 2s/step - loss: 1.2457\n",
      "Epoch 12/30\n",
      "243/243 [==============================] - 500s 2s/step - loss: 1.1946\n",
      "Epoch 13/30\n",
      "243/243 [==============================] - 430s 2s/step - loss: 1.1434\n",
      "Epoch 14/30\n",
      "243/243 [==============================] - 439s 2s/step - loss: 1.0918\n",
      "Epoch 15/30\n",
      "243/243 [==============================] - 429s 2s/step - loss: 1.0407\n",
      "Epoch 16/30\n",
      "243/243 [==============================] - 429s 2s/step - loss: 0.9919\n",
      "Epoch 17/30\n",
      "243/243 [==============================] - 434s 2s/step - loss: 0.9474\n",
      "Epoch 18/30\n",
      "243/243 [==============================] - 450s 2s/step - loss: 0.9071\n",
      "Epoch 19/30\n",
      "243/243 [==============================] - 442s 2s/step - loss: 0.8716\n",
      "Epoch 20/30\n",
      "243/243 [==============================] - 428s 2s/step - loss: 0.8413\n",
      "Epoch 21/30\n",
      "243/243 [==============================] - 442s 2s/step - loss: 0.8147\n",
      "Epoch 22/30\n",
      "243/243 [==============================] - 445s 2s/step - loss: 0.7933\n",
      "Epoch 23/30\n",
      "243/243 [==============================] - 434s 2s/step - loss: 0.7758\n",
      "Epoch 24/30\n",
      "243/243 [==============================] - 439s 2s/step - loss: 0.7612\n",
      "Epoch 25/30\n",
      "243/243 [==============================] - 464s 2s/step - loss: 0.7485\n",
      "Epoch 26/30\n",
      "243/243 [==============================] - 474s 2s/step - loss: 0.7390\n",
      "Epoch 27/30\n",
      "243/243 [==============================] - 454s 2s/step - loss: 0.7316\n",
      "Epoch 28/30\n",
      "243/243 [==============================] - 433s 2s/step - loss: 0.7208\n",
      "Epoch 29/30\n",
      "243/243 [==============================] - 428s 2s/step - loss: 0.7159\n",
      "Epoch 30/30\n",
      "243/243 [==============================] - 624s 3s/step - loss: 0.7110\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "lbHOC1K3kV5-"
   },
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "xdzwdYj-k4Nu"
   },
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "OU6G6gozk8Q8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stephen, he had murdered to.\r\n",
      "\r\n",
      "Alberrative parturine of eggliness had been knelt again with uncleaning pacts. She poured\r\n",
      "before slop, slips for words. Softly. Somewhere. He tops so go suddenly _voglio_. Damn your\r\n",
      "strength was more since Lord the day before neither for he was blue\r\n",
      "fellow, not ready to set him to pink year and saw it with means which he halted\r\n",
      "arphen sad Bloom whose requirements and personal congresslies that he was always for a\r\n",
      "work of fear not to hurt you see and saw at the king’s name was a country bad\r\n",
      "got off she went bling to how him? He’ll be all right she could wrinkled with her\r\n",
      "cashancy with creamine bundle burying, passed out.\r\n",
      "\r\n",
      "—Good Kinch and Joke’s head is, Mr Best said.\r\n",
      "\r\n",
      "The bowl mether in _Abbes_’s and Varelagend Bloom with his hat.\r\n",
      "Had he then he put the good in her necklike care and walking over it\r\n",
      "will half know by the wailing soupens or something to come back with\r\n",
      "drawn quiet thing like that. And Child’s daughter It lies in\r\n",
      "Gibraltar who thereun \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 2.2116405963897705\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['Stephen'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "mtkxHSv-k_lX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7f5b8400b0d0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step/assets\n"
     ]
    }
   ],
   "source": [
    "#save the trained model\n",
    "\n",
    "tf.saved_model.save(one_step_model, 'one_step')\n",
    "one_step_reloaded = tf.saved_model.load('one_step')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Ulysses.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
